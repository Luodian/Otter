{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "from peft.src.peft import LoraModel, LoraConfig\n",
    "import torch\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\", clip_vision_encoder_pretrained=\"openai\", lang_encoder_path=\"./llama-7b-hf\", tokenizer_path=\"./llama-7b-hf\", cross_attn_every_n_layers=4\n",
    ")\n",
    "\n",
    "checkpoint_path = \"/home/v-boli7/azure_storage/models/openflamingo/checkpoint.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "\n",
    "config = LoraConfig(\n",
    "    peft_type=\"LORA\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "\n",
    "lora_model = LoraModel(config, model)\n",
    "\n",
    "total_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters in LoRA is {total_params / 1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xb2\\xd9\\xd2\\xc5\\x99T=\\xec\\xd1\\xec{\\x8d\\x9b;\\xb1\\x143Q 7(\\x8d\\xa2\\x02\\xf5\\x98\\xa4\\x88\\x94\\x18s\\x8aM>o\\x13\\x0f\\xddVR\\xee\\xd63\\x9a\\xe1H\\x84C\\xf9vj\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 <\\xd7']\n",
      "Bad pipe message: %s [b'\\xa5\\xc8\\x8e\\xdf\\x8c\\x9a\\xfa\\xfb\\x89\\xc9\\x1c\\x97\\x17\\x9e\\x12lU\\xe7 \\xe4\\xd1\\x1a\\xa1`!\\xe8I\\xff\\x01\\xbeu6\\xc6z\\x18\\x87~\\x9dD\\x814\\xaatl\\tx\"\\x133\\x002\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00']\n",
      "Bad pipe message: %s [b'$\\x00\\x1d\\x00 w\\xd6\\xd7\\xad\\x90\\x91/\\x05\\xab\\xe9e\\xbc\\xb5\\xb6\\x16A\\x96m\\xd5\\xb2\\x8c\\xae\\x90J\\x97\\x0b\\x9a\\x86wMYd']\n",
      "Bad pipe message: %s [b'\\x99\\x83\\xf5N_v\\xb7f\\xfew\\xf5:$\\x06$X']\n",
      "Bad pipe message: %s [b\"\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\"]\n",
      "Bad pipe message: %s [b\"\\xcd\\x9f\\xea\\xb0%1G\\xe8\\xe8\\xa4{\\xdf\\x7f\\x9e\\xc1:\\xf6k\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0\", b'\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\xfe\\xeb\\xe4\\xbc\\x03\\xfe\\xb0\\x97y\\xa0\\x13\\xfc\\xc3C\\x11\\xb7\\xde\\xa6\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03']\n",
      "Bad pipe message: %s [b'Z\\x10C\\x93\\xde\\xbe\\xf9\\xc1\\xd6\\xe1\\xc7\\xa8n\\x07\\x91\\x00\\\\\\xdf\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17']\n",
      "Bad pipe message: %s [b'm\\x9f\\xec']\n",
      "Bad pipe message: %s [b\"\\x99\\x8f'\\xb9r\\xcf\\xe5\\x07\\x9ec\\x93\\xb3\\xbbX\\xe1dw6\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\"]\n",
      "Bad pipe message: %s [b\"\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\"]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavis.datasets.builders import dataset_zoo\n",
    "dataset_names = dataset_zoo.get_names()\n",
    "print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.datasets.builders import load_dataset\n",
    "\n",
    "coco_dataset = load_dataset(\"coco_caption\")\n",
    "coco_train_set = coco_dataset['train']\n",
    "for sample in coco_train_set:\n",
    "    print(sample)\n",
    "    print(sample['image'])\n",
    "    print(sample['text_input'])\n",
    "    break\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "coco_train_loader = torch.utils.data.DataLoader(coco_train_set, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "for batch in coco_train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aokvqa_dataset = load_dataset(\"aok_vqa\")\n",
    "aokvqa_train_set = aokvqa_dataset['train']\n",
    "for sample in aokvqa_train_set:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/vqa_train.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/vqa_val.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/vqa_val_eval.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/answer_list.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/v2_OpenEnded_mscoco_val2014_questions.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/v2_mscoco_val2014_annotations.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/vqa_test.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/coco/annotations/answer_list.json\n",
      "{'image': <PIL.Image.Image image mode=RGB size=640x480 at 0x7FF4D6BCBD00>, 'text_input': 'What is this photo taken looking through?', 'answers': ['net', 'netting', 'mesh'], 'weights': [0.7999999999999999, 0.1, 0.1]}\n"
     ]
    }
   ],
   "source": [
    "coco_vqa_dataset = load_dataset(\"coco_vqa\")\n",
    "coco_vqa_train_set = coco_vqa_dataset['train']\n",
    "for sample in coco_vqa_train_set:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/aokvqa/annotations/aokvqa_v1p0_train.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/aokvqa/annotations/aokvqa_v1p0_val.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/aokvqa/annotations/specialized_vocab_train_lavis.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/aokvqa/annotations/aokvqa_v1p0_test.json\n",
      "Using downloaded and verified file: /home/v-boli7/azure_storage/data/lavis/aokvqa/annotations/specialized_vocab_train_lavis.json\n",
      "dict_keys(['train', 'val', 'test'])\n",
      "17056\n",
      "{'image': <PIL.Image.Image image mode=RGB size=640x480 at 0x7FE2981CC790>, 'text_input': 'What is the man by the bags awaiting?', 'answers': ['ride', 'bus', 'taxi', 'travelling', 'traffic', 'cab', 'his ride'], 'weights': [0.2, 0.1, 0.2, 0.1, 0.1, 0.2, 0.1]}\n"
     ]
    }
   ],
   "source": [
    "from lavis.datasets.builders import load_dataset\n",
    "vqav2_dataset = load_dataset(\"aok_vqa\")\n",
    "print(vqav2_dataset.keys())\n",
    "print(len(vqav2_dataset[\"train\"]))\n",
    "print(vqav2_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_train.json to /home/v-boli7/azure_storage/data/lavis/okvqa/annotations/okvqa_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2530489/2530489 [00:00<00:00, 76175726.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_val_eval.json to /home/v-boli7/azure_storage/data/lavis/okvqa/annotations/vqa_val_eval.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1389419/1389419 [00:00<00:00, 38321873.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_answer_list_train.json to /home/v-boli7/azure_storage/data/lavis/okvqa/annotations/answer_list.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152278/152278 [00:00<00:00, 9876756.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_val2014_questions.json to /home/v-boli7/azure_storage/data/lavis/okvqa/annotations/OpenEnded_mscoco_val2014_questions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521744/521744 [00:00<00:00, 20632965.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_val2014_annotations.json to /home/v-boli7/azure_storage/data/lavis/okvqa/annotations/mscoco_val2014_annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8490544/8490544 [00:00<00:00, 12291234.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.Image.Image image mode=RGB size=640x479 at 0x7FE183330730>, 'text_input': 'What is the hairstyle of the blond called?', 'answers': ['pony tail', 'braid', 'ponytail'], 'weights': [0.6, 0.2, 0.2]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "okvqa_dataset = load_dataset(\"ok_vqa\")\n",
    "okvqa_train_set = okvqa_dataset['train']\n",
    "for sample in okvqa_train_set:\n",
    "    print(sample)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
