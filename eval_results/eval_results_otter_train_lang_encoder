================================================================================
                              EVALUATION REPORT
================================================================================


MODEL INFO: {'name': 'otter_image', 'model_path': '/mnt/petrelfs/zhangyuanhan/Otter/checkpoints/otter_llava_sft_nonconv_nogroup_train_lang_encoder/epoch_1/'}
--------------------------------------------------------------------------------
[2023-12-23 13:32:41,224] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Imported class: <class 'pipeline.benchmarks.models.otter_image.OtterImage'>
The current model version is configured for Otter-Image with max_num_frames set to None.
Unfreeze language decoder.
Parameter: lang_encoder.model.embed_tokens.weight, Size: 131.084288 M
Parameter: lang_encoder.model.layers.0.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.0.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.0.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.0.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.0.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.0.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.0.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.0.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.0.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.1.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.1.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.1.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.1.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.1.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.1.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.1.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.1.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.1.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.2.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.2.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.2.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.2.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.2.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.2.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.2.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.2.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.2.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.3.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.3.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.3.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.3.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.3.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.3.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.3.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.3.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.3.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.3.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.4.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.4.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.4.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.4.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.4.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.4.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.4.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.4.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.4.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.5.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.5.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.5.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.5.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.5.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.5.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.5.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.5.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.5.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.6.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.6.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.6.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.6.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.6.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.6.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.6.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.6.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.6.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.7.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.7.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.7.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.7.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.7.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.7.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.7.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.7.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.7.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.7.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.8.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.8.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.8.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.8.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.8.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.8.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.8.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.8.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.8.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.9.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.9.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.9.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.9.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.9.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.9.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.9.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.9.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.9.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.10.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.10.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.10.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.10.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.10.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.10.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.10.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.10.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.10.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.11.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.11.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.11.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.11.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.11.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.11.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.11.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.11.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.11.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.11.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.12.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.12.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.12.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.12.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.12.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.12.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.12.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.12.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.12.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.13.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.13.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.13.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.13.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.13.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.13.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.13.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.13.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.13.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.14.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.14.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.14.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.14.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.14.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.14.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.14.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.14.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.14.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.15.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.15.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.15.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.15.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.15.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.15.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.15.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.15.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.15.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.15.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.16.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.16.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.16.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.16.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.16.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.16.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.16.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.16.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.16.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.17.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.17.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.17.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.17.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.17.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.17.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.17.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.17.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.17.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.18.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.18.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.18.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.18.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.18.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.18.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.18.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.18.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.18.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.19.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.19.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.19.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.19.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.19.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.19.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.19.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.19.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.19.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.19.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.20.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.20.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.20.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.20.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.20.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.20.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.20.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.20.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.20.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.21.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.21.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.21.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.21.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.21.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.21.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.21.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.21.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.21.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.22.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.22.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.22.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.22.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.22.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.22.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.22.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.22.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.22.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.23.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.23.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.23.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.23.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.23.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.23.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.23.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.23.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.23.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.23.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.24.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.24.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.24.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.24.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.24.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.24.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.24.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.24.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.24.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.25.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.25.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.25.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.25.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.25.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.25.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.25.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.25.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.25.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.26.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.26.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.26.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.26.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.26.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.26.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.26.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.26.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.26.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.27.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.27.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.27.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.27.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.27.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.27.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.27.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.27.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.27.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.27.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.28.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.28.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.28.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.28.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.28.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.28.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.28.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.28.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.28.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.29.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.29.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.29.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.29.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.29.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.29.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.29.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.29.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.29.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.30.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.30.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.30.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.30.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.30.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.30.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.30.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.30.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.30.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.ff_gate, Size: 0.000001 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn.norm.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn.to_q.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn.to_kv.weight, Size: 1.048576 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.attn.to_out.weight, Size: 2.097152 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.feed_forward.0.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.feed_forward.0.bias, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.feed_forward.1.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.31.gated_cross_attn_layer.feed_forward.3.weight, Size: 67.108864 M
Parameter: lang_encoder.model.layers.31.decoder_layer.self_attn.q_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.31.decoder_layer.self_attn.k_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.31.decoder_layer.self_attn.v_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.31.decoder_layer.self_attn.o_proj.weight, Size: 16.777216 M
Parameter: lang_encoder.model.layers.31.decoder_layer.mlp.gate_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.31.decoder_layer.mlp.up_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.31.decoder_layer.mlp.down_proj.weight, Size: 45.088768 M
Parameter: lang_encoder.model.layers.31.decoder_layer.input_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.layers.31.decoder_layer.post_attention_layernorm.weight, Size: 0.004096 M
Parameter: lang_encoder.model.norm.weight, Size: 0.004096 M
Parameter: lang_encoder.lm_head.weight, Size: 131.084288 M
Parameter: perceiver.latents, Size: 0.065536 M
Parameter: perceiver.layers.0.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.0.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.0.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.0.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.0.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.0.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.0.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.0.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.0.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.0.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.0.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.layers.1.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.1.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.1.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.1.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.1.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.1.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.1.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.1.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.1.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.1.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.1.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.layers.2.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.2.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.2.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.2.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.2.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.2.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.2.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.2.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.2.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.2.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.2.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.layers.3.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.3.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.3.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.3.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.3.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.3.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.3.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.3.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.3.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.3.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.3.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.layers.4.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.4.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.4.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.4.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.4.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.4.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.4.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.4.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.4.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.4.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.4.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.layers.5.norm_media.weight, Size: 0.001024 M
Parameter: perceiver.layers.5.norm_media.bias, Size: 0.001024 M
Parameter: perceiver.layers.5.norm_latents.weight, Size: 0.001024 M
Parameter: perceiver.layers.5.norm_latents.bias, Size: 0.001024 M
Parameter: perceiver.layers.5.to_q.weight, Size: 0.524288 M
Parameter: perceiver.layers.5.to_kv.weight, Size: 1.048576 M
Parameter: perceiver.layers.5.to_out.weight, Size: 0.524288 M
Parameter: perceiver.layers.5.feed_forward.0.weight, Size: 0.001024 M
Parameter: perceiver.layers.5.feed_forward.0.bias, Size: 0.001024 M
Parameter: perceiver.layers.5.feed_forward.1.weight, Size: 4.194304 M
Parameter: perceiver.layers.5.feed_forward.3.weight, Size: 4.194304 M
Parameter: perceiver.norm.weight, Size: 0.001024 M
Parameter: perceiver.norm.bias, Size: 0.001024 M
Total Trainable param: 7.917275 B
Imported class: <class 'pipeline.benchmarks.datasets.mme.MMEDataset'>

DATASET: MMEDataset
--------------------
=========== Cognition ===========
total score: 304.6428571428571
	 code_reasoning score: 65.0
	 numerical_calculation score: 50.0
	 text_translation score: 97.5
	 commonsense_reasoning score: 92.14285714285714
=========== Perception ===========
total score: 995.9196678671468
	 artwork score: 67.0
	 celebrity score: 76.17647058823529
	 count score: 96.66666666666666
	 color score: 70.0
	 position score: 56.666666666666664
	 OCR score: 102.5
	 landmark score: 113.75
	 scene score: 138.5
	 existence score: 175.0
	 posters score: 99.65986394557824

--------------------------------------------------------------------------------
Total Datasets Evaluated: 1

================================================================================
