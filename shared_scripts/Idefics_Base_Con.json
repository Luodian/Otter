{
	"_name_or_path": "None",
	"additional_vocab_size": 2,
	"alpha_initializer": "zeros",
	"alpha_type": "float",
	"alphas_initializer_range": 0.0,
	"architectures": ["IdeficsForVisionText2Text"],
	"bos_token_id": 1,
	"cross_layer_activation_function": "swiglu",
	"cross_layer_interval": 4,
	"dropout": 0.0,
	"eos_token_id": 2,
	"freeze_lm_head": false,
	"freeze_text_layers": true,
	"freeze_text_module_exceptions": [],
	"freeze_vision_layers": true,
	"freeze_vision_module_exceptions": [],
	"hidden_act": "silu",
	"hidden_size": 4096,
	"initializer_range": 0.02,
	"intermediate_size": 11008,
	"max_position_embeddings": 2048,
	"max_sequence_length": 2048,
	"model_type": "idefics",
	"num_attention_heads": 32,
	"num_hidden_layers": 32,
	"pad_token_id": 0,
	"qk_layer_norms": true,
	"rms_norm_eps": 1e-6,
	"tie_word_embeddings": false,
	"torch_dtype": "bfloat16",
	"transformers_version": "4.28.0.dev0",
	"use_cache": true,
	"use_resampler": true,
	"vocab_size": 32000,
	"vision_config": {
		"hidden_act": "gelu",
		"embed_dim": 1280,
		"image_size": 224,
		"intermediate_size": 5120,
		"patch_size": 14,
		"num_attention_heads": 16,
		"num_hidden_layers": 32
	},
	"perceiver_config": {
		"qk_layer_norms_perceiver": true,
		"resampler_depth": 6,
		"resampler_head_dim": 96,
		"resampler_n_heads": 16,
		"resampler_n_latents": 64
	}
}
